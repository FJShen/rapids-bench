Picked up JAVA_TOOL_OPTIONS: 
Picked up JAVA_TOOL_OPTIONS: 
Warning: Ignoring non-Spark config property: parquet.memory.pool.ratio
21/07/27 17:18:52 WARN Utils: Your hostname, tgrogers-pc01 resolves to a loopback address: 127.0.1.1; using 128.46.76.221 instead (on interface enp6s0)
21/07/27 17:18:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
21/07/27 17:18:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
21/07/27 17:18:55 INFO SparkContext: Running Spark version 3.0.0
21/07/27 17:18:55 INFO ResourceUtils: ==============================================================
21/07/27 17:18:55 INFO ResourceUtils: Resources for spark.driver:

21/07/27 17:18:55 INFO ResourceUtils: ==============================================================
21/07/27 17:18:55 INFO SparkContext: Submitted application: TPC-H Like Bench q3
21/07/27 17:18:56 INFO SecurityManager: Changing view acls to: shen449
21/07/27 17:18:56 INFO SecurityManager: Changing modify acls to: shen449
21/07/27 17:18:56 INFO SecurityManager: Changing view acls groups to: 
21/07/27 17:18:56 INFO SecurityManager: Changing modify acls groups to: 
21/07/27 17:18:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(shen449); groups with view permissions: Set(); users  with modify permissions: Set(shen449); groups with modify permissions: Set()
21/07/27 17:18:56 INFO Utils: Successfully started service 'sparkDriver' on port 35135.
21/07/27 17:18:57 INFO SparkEnv: Registering MapOutputTracker
21/07/27 17:18:57 INFO SparkEnv: Registering BlockManagerMaster
21/07/27 17:18:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
21/07/27 17:18:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
21/07/27 17:18:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
21/07/27 17:18:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1bcf6a6a-89b0-4f80-b0d6-99aa8639dbd0
21/07/27 17:18:57 INFO MemoryStore: MemoryStore started with capacity 16.9 GiB
21/07/27 17:18:57 INFO SparkEnv: Registering OutputCommitCoordinator
21/07/27 17:18:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
21/07/27 17:18:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://tgrogers-pc01.ecn.purdue.edu:4040
21/07/27 17:18:59 INFO SparkContext: Added JAR file:///home/shen449/rapids-bench/lib/rapids-4-spark_2.12-0.3.0.jar at spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/rapids-4-spark_2.12-0.3.0.jar with timestamp 1627420739531
21/07/27 17:18:59 INFO SparkContext: Added JAR file:///home/shen449/rapids-bench/lib/cudf-0.17.jar at spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/cudf-0.17.jar with timestamp 1627420739534
21/07/27 17:18:59 INFO SparkContext: Added JAR file:///home/shen449/rapids-bench/lib/scallop_2.12-3.5.1.jar at spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/scallop_2.12-3.5.1.jar with timestamp 1627420739534
21/07/27 17:18:59 INFO SparkContext: Added JAR file:/home/shen449/rapids-bench/lib/rapids-4-spark-integration-tests_2.12-0.3.0.jar at spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/rapids-4-spark-integration-tests_2.12-0.3.0.jar with timestamp 1627420739534
21/07/27 17:19:00 INFO Executor: Starting executor ID driver on host tgrogers-pc01.ecn.purdue.edu
21/07/27 17:19:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43011.
21/07/27 17:19:00 INFO NettyBlockTransferService: Server created on tgrogers-pc01.ecn.purdue.edu:43011
21/07/27 17:19:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
21/07/27 17:19:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, tgrogers-pc01.ecn.purdue.edu, 43011, None)
21/07/27 17:19:00 INFO BlockManagerMasterEndpoint: Registering block manager tgrogers-pc01.ecn.purdue.edu:43011 with 16.9 GiB RAM, BlockManagerId(driver, tgrogers-pc01.ecn.purdue.edu, 43011, None)
21/07/27 17:19:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, tgrogers-pc01.ecn.purdue.edu, 43011, None)
21/07/27 17:19:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, tgrogers-pc01.ecn.purdue.edu, 43011, None)
21/07/27 17:19:00 INFO SingleEventLogFileWriter: Logging events to file:/home/shen449/rapids-bench/spark-event-logs/local-1627420739772.inprogress
21/07/27 17:19:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/shen449/rapids-bench/spark-warehouse').
21/07/27 17:19:02 INFO SharedState: Warehouse path is 'file:/home/shen449/rapids-bench/spark-warehouse'.
21/07/27 17:19:06 INFO InMemoryFileIndex: It took 217 ms to list leaf files for 1 paths.
21/07/27 17:19:07 INFO SparkContext: Starting job: parquet at TpchLikeSpark.scala:123
21/07/27 17:19:07 INFO DAGScheduler: Got job 0 (parquet at TpchLikeSpark.scala:123) with 1 output partitions
21/07/27 17:19:07 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at TpchLikeSpark.scala:123)
21/07/27 17:19:07 INFO DAGScheduler: Parents of final stage: List()
21/07/27 17:19:07 INFO DAGScheduler: Missing parents: List()
21/07/27 17:19:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at TpchLikeSpark.scala:123), which has no missing parents
21/07/27 17:19:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 84.7 KiB, free 16.9 GiB)
21/07/27 17:19:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.9 KiB, free 16.9 GiB)
21/07/27 17:19:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on tgrogers-pc01.ecn.purdue.edu:43011 (size: 29.9 KiB, free: 16.9 GiB)
21/07/27 17:19:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
21/07/27 17:19:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at TpchLikeSpark.scala:123) (first 15 tasks are for partitions Vector(0))
21/07/27 17:19:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
21/07/27 17:19:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, tgrogers-pc01.ecn.purdue.edu, executor driver, partition 0, PROCESS_LOCAL, 7570 bytes)
21/07/27 17:19:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
21/07/27 17:19:09 INFO Executor: Fetching spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/rapids-4-spark_2.12-0.3.0.jar with timestamp 1627420739531
21/07/27 17:19:09 INFO TransportClientFactory: Successfully created connection to tgrogers-pc01.ecn.purdue.edu/128.46.76.221:35135 after 169 ms (0 ms spent in bootstraps)
21/07/27 17:19:09 INFO Utils: Fetching spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/rapids-4-spark_2.12-0.3.0.jar to /tmp/spark-1b197fad-b1e2-4288-9f32-04428724bc28/userFiles-239e5ca4-ff35-4093-b2c4-ce20d478a693/fetchFileTemp162628849267785892.tmp
21/07/27 17:19:09 INFO Executor: Adding file:/tmp/spark-1b197fad-b1e2-4288-9f32-04428724bc28/userFiles-239e5ca4-ff35-4093-b2c4-ce20d478a693/rapids-4-spark_2.12-0.3.0.jar to class loader
21/07/27 17:19:09 INFO Executor: Fetching spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/scallop_2.12-3.5.1.jar with timestamp 1627420739534
21/07/27 17:19:09 INFO Utils: Fetching spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/scallop_2.12-3.5.1.jar to /tmp/spark-1b197fad-b1e2-4288-9f32-04428724bc28/userFiles-239e5ca4-ff35-4093-b2c4-ce20d478a693/fetchFileTemp6649231582170443010.tmp
21/07/27 17:19:09 INFO Executor: Adding file:/tmp/spark-1b197fad-b1e2-4288-9f32-04428724bc28/userFiles-239e5ca4-ff35-4093-b2c4-ce20d478a693/scallop_2.12-3.5.1.jar to class loader
21/07/27 17:19:09 INFO Executor: Fetching spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/rapids-4-spark-integration-tests_2.12-0.3.0.jar with timestamp 1627420739534
21/07/27 17:19:09 INFO Utils: Fetching spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/rapids-4-spark-integration-tests_2.12-0.3.0.jar to /tmp/spark-1b197fad-b1e2-4288-9f32-04428724bc28/userFiles-239e5ca4-ff35-4093-b2c4-ce20d478a693/fetchFileTemp3410608852958848775.tmp
21/07/27 17:19:09 INFO Executor: Adding file:/tmp/spark-1b197fad-b1e2-4288-9f32-04428724bc28/userFiles-239e5ca4-ff35-4093-b2c4-ce20d478a693/rapids-4-spark-integration-tests_2.12-0.3.0.jar to class loader
21/07/27 17:19:09 INFO Executor: Fetching spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/cudf-0.17.jar with timestamp 1627420739534
21/07/27 17:19:09 INFO Utils: Fetching spark://tgrogers-pc01.ecn.purdue.edu:35135/jars/cudf-0.17.jar to /tmp/spark-1b197fad-b1e2-4288-9f32-04428724bc28/userFiles-239e5ca4-ff35-4093-b2c4-ce20d478a693/fetchFileTemp6747997606744287592.tmp
21/07/27 17:19:20 ERROR TransportRequestHandler: Error sending result StreamResponse[streamId=/jars/cudf-0.17.jar,byteCount=287360437,body=FileSegmentManagedBuffer[file=/home/shen449/rapids-bench/lib/cudf-0.17.jar,offset=0,length=287360437]] to /128.46.76.221:51712; closing connection
java.io.IOException: Input/output error
	at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
	at sun.nio.ch.FileChannelImpl.transferToDirectlyInternal(FileChannelImpl.java:428)
	at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:493)
	at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:605)
	at io.netty.channel.DefaultFileRegion.transferTo(DefaultFileRegion.java:130)
	at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:121)
	at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:362)
	at io.netty.channel.nio.AbstractNioByteChannel.doWriteInternal(AbstractNioByteChannel.java:235)
	at io.netty.channel.nio.AbstractNioByteChannel.doWrite0(AbstractNioByteChannel.java:209)
	at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:400)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:930)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.forceFlush(AbstractNioChannel.java:361)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:708)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/07/27 17:19:20 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.StreamInterceptor.channelInactive(StreamInterceptor.java:62)
	at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:223)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:818)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/07/27 17:19:21 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, tgrogers-pc01.ecn.purdue.edu, executor driver): java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.StreamInterceptor.channelInactive(StreamInterceptor.java:62)
	at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:223)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:818)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

21/07/27 17:19:21 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
21/07/27 17:19:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
21/07/27 17:19:21 INFO TaskSchedulerImpl: Cancelling stage 0
21/07/27 17:19:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
21/07/27 17:19:21 INFO DAGScheduler: ResultStage 0 (parquet at TpchLikeSpark.scala:123) failed in 13.082 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, tgrogers-pc01.ecn.purdue.edu, executor driver): java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.StreamInterceptor.channelInactive(StreamInterceptor.java:62)
	at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:223)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:818)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
21/07/27 17:19:21 INFO DAGScheduler: Job 0 failed: parquet at TpchLikeSpark.scala:123, took 13.504641 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, tgrogers-pc01.ecn.purdue.edu, executor driver): java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.StreamInterceptor.channelInactive(StreamInterceptor.java:62)
	at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:223)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:818)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1003)
	at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:67)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:493)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:107)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:163)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:193)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:190)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:401)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:279)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:268)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:268)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:737)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:715)
	at com.nvidia.spark.rapids.tests.tpch.TpchLikeSpark$.setupOrdersParquet(TpchLikeSpark.scala:123)
	at com.nvidia.spark.rapids.tests.tpch.TpchLikeSpark$.setupAllParquet(TpchLikeSpark.scala:82)
	at com.nvidia.spark.rapids.tests.tpch.TpchLikeBench$.setupAllParquet(TpchLikeBench.scala:29)
	at com.nvidia.spark.rapids.tests.BenchmarkRunner$.main(BenchmarkRunner.scala:58)
	at com.nvidia.spark.rapids.tests.BenchmarkRunner.main(BenchmarkRunner.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.nio.channels.ClosedChannelException
	at org.apache.spark.network.client.StreamInterceptor.channelInactive(StreamInterceptor.java:62)
	at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:223)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:818)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
21/07/27 17:19:21 INFO SparkContext: Invoking stop() from shutdown hook
21/07/27 17:19:21 INFO SparkUI: Stopped Spark web UI at http://tgrogers-pc01.ecn.purdue.edu:4040
21/07/27 17:19:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
21/07/27 17:19:21 INFO MemoryStore: MemoryStore cleared
21/07/27 17:19:21 INFO BlockManager: BlockManager stopped
21/07/27 17:19:21 INFO BlockManagerMaster: BlockManagerMaster stopped
21/07/27 17:19:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
21/07/27 17:19:21 INFO SparkContext: Successfully stopped SparkContext
21/07/27 17:19:21 INFO ShutdownHookManager: Shutdown hook called
21/07/27 17:19:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b197fad-b1e2-4288-9f32-04428724bc28
21/07/27 17:19:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-a0968ba1-b80f-42c6-9ea6-68fdcb8cce16
$SPARK_HOME/bin/spark-submit \
--master local[*] \
--conf spark.eventLog.enabled=true \
--conf spark.eventLog.dir=./spark-event-logs \
  --conf spark.app.name="tpch-cpu" --conf spark.driver.memory=32G --conf spark.executor.memory=16g --conf spark.sql.parquet.compression.codec=none --conf spark.sql.files.maxRecordsPerFile=20000000 --conf parquet.memory.pool.ratio=0.5 --conf spark.executor.instances=1 --conf spark.executor.cores=20 --conf spark.locality.wait=0s --conf spark.sql.files.maxPartitionBytes=671088640 --jars $SPARK_RAPIDS_PLUGIN_JAR,$CUDF_JAR,$SCALLOP_JAR --class com.nvidia.spark.rapids.tests.BenchmarkRunner $SPARK_RAPIDS_PLUGIN_INTEGRATION_TEST_JAR --benchmark tpch --query q3 --input ./tpch-tables/1_none/ --input-format parquet --summary-file-prefix tpch-cpu --iterations 2
